{
  "integration_tasks": {
    "tool_loader_system": {
      "description": "Automated tool loading and registration system",
      "files_to_create": {
        "loaders/tool_discovery.py": "Auto-discover all tools in codebase",
        "loaders/tool_loader.py": "Load tools into tool_registry table",
        "loaders/tool_validator.py": "Validate tool schemas and functionality",
        "loaders/bulk_loader.py": "Bulk load all tools with one command",
        "loaders/tool_manifest_generator.py": "Generate tool manifests from code"
      },
      "requirements": [
        "Scan tools/ directory and extract metadata from docstrings",
        "Generate proper JSON schema for each tool",
        "Insert into tool_registry with correct gateway_type",
        "Validate each tool works before registration",
        "Support hot-reloading of tools without restart"
      ]
    },
    
    "litellm_gateway_setup": {
      "description": "Complete LiteLLM gateway configuration",
      "files_to_create": {
        "gateways/litellm_config_generator.py": "Generate litellm_config.yaml from database",
        "gateways/litellm_gateway.py": "LiteLLM gateway implementation",
        "gateways/model_router.py": "Intelligent model routing based on task",
        "gateways/cost_tracker.py": "Track API costs per model/user",
        "gateways/fallback_handler.py": "Handle model failures with fallbacks"
      },
      "requirements": [
        "Read model_registry table and generate LiteLLM config",
        "Support all models: OpenAI, Anthropic, Google, local",
        "Implement smart routing based on task complexity",
        "Track costs and enforce quotas",
        "Automatic fallback to cheaper models on failure",
        "Function calling support for all models"
      ],
      "litellm_config_template": {
        "model_list": [
          {
            "model_name": "gpt-4.1",
            "litellm_params": {
              "model": "openai/gpt-4o-mini",
              "api_key": "os.environ/OPENAI_API_KEY",
              "max_tokens": 4096
            }
          },
          {
            "model_name": "claude-4-opus",
            "litellm_params": {
              "model": "anthropic/claude-3-opus-20240229",
              "api_key": "os.environ/ANTHROPIC_API_KEY"
            }
          },
          {
            "model_name": "gemini-2.5-pro",
            "litellm_params": {
              "model": "vertex_ai/gemini-2.5-pro",
              "vertex_project": "os.environ/GOOGLE_CLOUD_PROJECT",
              "vertex_location": "us-central1"
            }
          }
        ],
        "router_settings": {
          "routing_strategy": "cost-optimized",
          "enable_fallbacks": true,
          "retry_policy": {
            "max_retries": 3,
            "retry_delay": 1
          }
        }
      }
    },
    
    "supabase_edge_functions": {
      "description": "Edge Functions for real-time processing",
      "files_to_create": {
        "edge-functions/job-processor/index.ts": "Process jobs from command_queue",
        "edge-functions/webhook-handler/index.ts": "Handle incoming webhooks",
        "edge-functions/realtime-notifier/index.ts": "Send real-time notifications",
        "edge-functions/api-gateway/index.ts": "Edge API gateway with auth",
        "edge-functions/vector-search/index.ts": "Vector similarity search",
        "edge-functions/graph-query/index.ts": "Graph database queries",
        "edge-functions/ml-inference/index.ts": "Run ML inference at edge"
      },
      "requirements": [
        "Each function auto-deploys to Supabase",
        "Connects to database using service role key",
        "Implements proper error handling and retries",
        "Sends metrics to monitoring system",
        "Supports WebSocket connections for real-time"
      ],
      "deployment_script": "deploy-edge-functions.sh"
    },
    
    "claude_mcp_integration": {
      "description": "MCP server for Claude integration",
      "files_to_create": {
        "mcp/claude_mcp_server.py": "MCP server exposing all tools",
        "mcp/tool_wrapper.py": "Wrap database tools for MCP",
        "mcp/auth_handler.py": "Handle MCP authentication",
        "mcp/session_manager.py": "Manage Claude sessions"
      },
      "requirements": [
        "Expose all tools from tool_registry via MCP",
        "Handle Claude's function calling format",
        "Maintain session state between calls",
        "Stream responses for long operations"
      ]
    },
    
    "config_management": {
      "description": "Dynamic configuration management",
      "files_to_create": {
        "config/config_loader.py": "Load all configs from database",
        "config/env_generator.py": "Generate .env from database",
        "config/secret_manager.py": "Manage secrets securely",
        "config/feature_flags.py": "Feature flag system",
        "config/dynamic_updater.py": "Update configs without restart"
      },
      "requirements": [
        "All configuration stored in database tables",
        "Generate environment files for legacy tools",
        "Encrypt secrets at rest",
        "Support A/B testing with feature flags",
        "Hot-reload configuration changes"
      ]
    },
    
    "startup_orchestrator": {
      "description": "Master startup script that initializes everything",
      "files_to_create": {
        "scripts/startup.py": "Main startup orchestrator",
        "scripts/health_checker.py": "Verify all services healthy",
        "scripts/dependency_installer.py": "Install missing dependencies",
        "scripts/database_initializer.py": "Initialize database if empty",
        "scripts/service_starter.py": "Start all services in order"
      },
      "requirements": [
        "Single command to start entire system",
        "Check and install dependencies",
        "Initialize database with schemas",
        "Load all tools into registry",
        "Start all services with health checks",
        "Generate status dashboard URL"
      ]
    }
  },
  
  "final_integration_prompt": "Create a complete integration layer that:\n\n1. AUTO-DISCOVERS all tools and loads them into tool_registry\n2. CONFIGURES LiteLLM gateway reading from model_registry\n3. DEPLOYS Supabase Edge Functions for real-time processing\n4. SETS UP MCP server for Claude integration\n5. IMPLEMENTS dynamic configuration from database\n6. CREATES master startup script for one-command deployment\n\nThe system should be able to:\n- Start with empty database\n- Run startup.py\n- Have everything configured and running\n- All tools available via REST API, GraphQL, CLI, and MCP\n\nEnsure LiteLLM config supports:\n- All models from model_registry\n- Cost tracking and quotas\n- Intelligent routing\n- Function calling for tool execution\n- Fallback chains for reliability",
  
  "tool_loading_sql": "-- After all tools are loaded, you'll have:\nSELECT COUNT(*) FROM tool_registry; -- Should show 50+ tools\n\n-- Categories include:\n-- 'analysis' - NER, embeddings, etc\n-- 'database' - Schema extraction, migration\n-- 'pipeline' - Import/export tools\n-- 'ml' - Model inference, evaluation\n-- 'monitoring' - Metrics, health checks\n-- 'security' - Encryption, validation\n-- 'workflow' - DAG execution, scheduling"
}